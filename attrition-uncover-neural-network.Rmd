---
title: "Employee Attrition Uncover"
author: "Alfan"
date: "4/1/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro 

## Background

This is a fictional data set created by IBM data scientists. The target why they create this fictial data set, is to uncover the factors that lead to employee attrition. 

In this case i would use specific question is, How to classify using neural network method and how is result of our model and predict classify and compare with our data test. Beside it we would try to uncover which feature is important to understand employee attrition

## Dataset

Dataset we get from this [kaggle](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset), this is an public dataset which mean everyone can access dataset.


# Data Preparation

## Import library

In this step, we would like to preparing and wrangling data to help do this process and to help our modelling we need to import some package. We will use some wrangling package, some data processing and modelling package, and we need some visualize package.

```{r message=FALSE}
library(tidyverse)
library(ggplot2)
library(GGally)
library(reshape)
library(keras)
library(rsample)
library(recipes)
library(yardstick)
library(caret)
library(plotly)
library(lime)
library(scales)
library(corrr)
library(tidyquant)

options(scipen = 100)
```

## Read Data

Import the dataset from CSV

```{r}
employee <- read.csv("data/WA_Fn-UseC_-HR-Employee-Attrition.csv")
```

```{r}
glimpse(employee)
```

# Data Wrangling

After import data we try to preview the data we have now. I use `summary` function to get summary from each feature

```{r}
summary(employee)
```

We have refrences from sourc that some feature containt leveling classification:    

Education   
  1. 'Below College'   
  2. 'College'   
  3. 'Bachelor'    
  4. 'Master'    
  5. 'Doctor'    

EnvironmentSatisfaction
  1. 'Low'    
  2. 'Medium'    
  3. 'High'    
  4. 'Very High'    

JobInvolvement
  1. 'Low'    
  2. 'Medium'   
  3. 'High'    
  4. 'Very High'    

JobSatisfaction    
  1. 'Low'    
  2. 'Medium'    
  3. 'High'    
  4. 'Very High'    

PerformanceRating    
  1. 'Low'    
  2. 'Good'    
  3. 'Excellent'    
  4. 'Outstanding'    
 
RelationshipSatisfaction    
  1. 'Low'    
  2. 'Medium'    
  3. 'High'    
  4. 'Very High'    

WorkLifeBalance    
  1. 'Bad'    
  2. 'Good'    
  3. 'Better'    
  4. 'Best'    

Based on refrence abouve some of our feature have missmatch data type, so we will tro to convert some feature to be correct data type.    

```{r}
employee <- employee %>%
  mutate(
    Education = as.factor(Education),
    EnvironmentSatisfaction = as.factor(EnvironmentSatisfaction),
    JobInvolvement = as.factor(JobInvolvement),
    JobLevel = as.factor(JobLevel),
    JobSatisfaction = as.factor(JobSatisfaction),
    PerformanceRating = as.factor(PerformanceRating),
    RelationshipSatisfaction = as.factor(RelationshipSatisfaction),
    StockOptionLevel = as.factor(StockOptionLevel),
    WorkLifeBalance = as.factor(WorkLifeBalance)
  )

summary(employee)
```

`Attrition` is our target variable. we would set 'Yes' label as our positive value, means our priority target is to know which observation get 'Yes' label. We need to set the levels inside the structure data 'Yes' level more higher than 'No'.

```{r}
employee <- employee %>% 
  mutate(
    Attrition = factor(Attrition, levels = c("Yes", "No"))
  )
```

Check NA or missing value value inside the datase

```{r}
table(is.na(employee))
```

Result is we dont have any NA or Missing value inside our dataset, Finally, we get our final starting dataset

```{r}
str(employee)
```

# Exploratory Data Analysis 

We will observe if there is class imbalance by looking proportion of target variable `Attrition`

```{r}
prop.table(table(employee$Attrition))
```

Found there is imblance class in our target variable, which 'Yes' class is bigger than 'No' class and it really have big gap 83% compare 16%.     

Based on this findings we need down or upsampling the dataset.

# Modelling 

## Cross-Validation

We will split the data into training set, validation set, and testing set. first step we need to split dataset to be training and testing dataset.     

```{r}
set.seed(100)
initial_split <- initial_split(employee, prop = 0.8, strata = "Attrition")

set.seed(100)
train_split <- initial_split(training(initial_split), prop = 0.8, strata = "Attrition")
```

We will split our training dataset to be training dataset and validation dataset with proportion training dataset around 80% for training dan 20% for validation. Beside it we would downsample data

```{r}
rec <- recipe(Attrition ~ ., training(train_split)) %>% 
  step_rm(StandardHours, EmployeeCount, EmployeeNumber, Over18) %>% 
  step_nzv(all_predictors()) %>% 
  step_upsample(Attrition, ratio = 1/1, seed = 100) %>% 
  step_range(all_numeric(), min = 0, max = 1, -Attrition) %>%
  # step_center(all_numeric()) %>%
  # step_scale(all_numeric()) %>%
  step_dummy(all_nominal(), -Attrition, one_hot = FALSE) %>% 
  prep(strings_as_factors = FALSE)

data_train <- juice(rec)
data_val <- bake(rec, testing(train_split))
data_test <- bake(rec, testing(initial_split))

```

```{r}
initial_split
prop.table(table(data_train$Attrition))
prop.table(table(data_val$Attrition))
prop.table(table(data_test$Attrition))
```

We can see we downsample only data train and let real data for data validation and data test. We adjust the data to get a proper structure before we fed them into keras.     

```{r}
train_y <- as.numeric(data_train$Attrition)-1
train_x <- data_train %>% 
  select(-Attrition) %>% 
  data.matrix()

val_y <- as.numeric(data_val$Attrition)-1
val_x <- data_val %>% 
  select(-Attrition) %>% 
  data.matrix()

test_y <- as.numeric(data_test$Attrition)-1
test_x <- data_test %>% 
  select(-Attrition) %>% 
  data.matrix()
```


## Neural Network

### Architecture: Multilayer Perceptron

Neural Network is inspired by the biological neural network system of our brain. It consists of input layer, hidden layer, and output layer. The data will be fed into the input layer, processed through the hidden layer, and converted into specific values, such as probability, in the output layer. The MLP has a back-propagation feature, which means that it will go back and forth to adjust the weight of each connection between neurons in order to minimize the loss function and get better performance.

```{r fig.align="center", echo=FALSE}
knitr::include_graphics("./images/nn-architecture.png")
```

We will build several layers. There are layer dense which will scale our data using `relu` activation function in the first and second layer. I will put drouput layer to prevent the model from overfitting. For last layer, we scale back our dat int range [0,1] with `sigmoid` activation function asthe probability of our data belong to a particular class.

```{r}
input_n <- ncol(train_x)

model <- keras_model_sequential() %>%
  layer_dense(input_shape = input_n,
              units = 32,
              activation = "relu") %>%
  layer_dense(units = 16,
              activation = "relu") %>%
  # layer_dropout(rate = 0.1) %>%
  # layer_batch_normalization() %>%
  layer_dense(units = 1,
              activation = "sigmoid")

model %>%
  compile(optimizer = "adam",
          metric = "accuracy",
          loss = "binary_crossentropy")

model
```

### Model Fitting

```{r}
set.seed(100)

history <- model %>%
  fit(
    x = train_x,
    y = train_y,
    batch_size = 124,
    epochs = 10,
    seed = 100,
    verbose = 1,
    validation_data = list(
      val_x,
      val_y
    )
  )

plot(history)
```

Our Model get 80% accuracy on training dataset and 67% accuracy on validation dataset. We get the difference between it about 13% it still accaptable and can conclude that our model we made before isnt overfit.

# Model Evaluation

## Performance

```{r}
pred_test <- as_tibble(predict(model, test_x)) %>%
  set_names("value") %>%
  mutate(class = if_else(value > 0.5, "No", "Yes")) %>%
  mutate(class = factor(class, levels = levels(data_test$Attrition))) %>%
  set_names(paste0("pred_", colnames(.)))

pred_test <- data_test %>%
  select(Attrition) %>%
  bind_cols(pred_test)

summary(pred_test$pred_class)

pred_test
```

We will check confusion matrix from test dataset.

```{r}
pred_test %>%
  conf_mat(Attrition, pred_class) %>%
  autoplot(type = "heatmap")
```


```{r}
# metrics summary
pred_test %>%
  summarise(
    accuracy = accuracy_vec(Attrition, pred_class),
    sensitivity = sens_vec(Attrition, pred_class),
    specificity = spec_vec(Attrition, pred_class),
    precision = precision_vec(Attrition, pred_class)
  )
```

## Roc Curve

```{r}
pred_test %>%
  roc_curve(Attrition, pred_value) %>%
  autoplot()
```


```{r}
pred_test %>% 
  roc_auc(Attrition, pred_value)
```

## Sensitivity - Specificty Curve

```{r}
pred_test_roc <- pred_test %>%
  roc_curve(Attrition, pred_value)

p <- pred_test_roc %>%
  mutate_if(~ is.numeric(.), ~ round(.,4)) %>%
  gather(metric, value, -.threshold) %>%
  ggplot(aes(.threshold, value)) +
  geom_line(aes(colour = metric)) +
  labs(x = "Probability Threshold to be Classified as Positive", y = "Value", colour = "Metrics") +
  theme_minimal()

ggplotly(p)
```

## Precision - Recall Curve

```{r}
pred_test %>%
  pr_curve(Attrition, pred_value) %>%
  autoplot()
```

```{r}
pred_test_pr <- pred_test %>%
  pr_curve(Attrition, pred_value)

p <- pred_test_pr %>%
  mutate_if(~ is.numeric(.), ~ round(.,4)) %>%
  gather(metric, value, -.threshold) %>%
  ggplot(aes(.threshold, value)) +
  geom_line(aes(colour = metric)) +
  labs(x = "Probability Threshold to be Classified as Positive", y = "Value", colour = "Metrics") +
  theme_minimal()

ggplotly(p)

```

# Model Audit

This process target is to understand how our model works to decide classification to our each observant. Why we need it? because Neural networks is “black box” nature meaning these sophisticated models. I will use `LIME` packages to intepret how it works.

```{r warning=FALSE}
# choose explanation data
data_explain <- testing(initial_split)

get_features <- function(x) {
  matrix <- data.matrix(bake(rec, x, -Attrition))
  matrix
}

lime_model <- as_classifier(model, labels = levels(data_explain$Attrition))

set.seed(100)
explainer <- lime(
  x = data_explain,
  model = lime_model,
  preprocess = get_features
)

# get lime explanation
explanation <- explain(
  x = data_explain[1:4,],
  explainer = explainer,
  n_labels = 1,
  n_features = 4
)

# plot feature explanation

plot_features(explanation) + 
  labs(title = "LIME Feature Importance Visualization")
```

Plot above i use `LIME` package to understand and get which feature are importance to our model decide the classification. This packages allows us to visualize each of the first 4 cases (observations) from the test data. The top four features for each case are shown. Note that they are not the same for each case. The blue bars mean that the feature supports the model conclusion, and the red bars contradict. A few important features based on frequency in first 4 cases:    

  * Work Life Balance
  * Business Travel Frequency
  * Martial Status    
  
One thing we need to be careful with the LIME visualization is that we are only doing a sample of the data, in our case the first 4 test observations. Therefore, we are gaining a very localized understanding of how our models work. However, we also want to know on from a global perspective what drives feature importance.

We can perform a correlation analysis on the training set as well to help glean what features correlate globally to “Attrition”. We’ll use the `corrr` package, which performs tidy correlations:    

```{r}
# Feature correlations to Churn
corrr_analysis <- data.frame(train_x) %>%
  mutate(Attrition = train_y) %>%
  correlate() %>%
  focus(Attrition) %>%
  rename(feature = rowname) %>%
  arrange(abs(Attrition)) %>%
  mutate(feature = as_factor(feature)) 

corrr_analysis <- corrr_analysis %>% 
  mutate(absAttrition = abs(Attrition)) %>% 
  arrange(desc(absAttrition)) %>% 
  slice(1:20) %>% 
  select(-absAttrition)

corrr_analysis
```

```{r}
# Correlation visualization
corrr_analysis %>%
  ggplot(aes(x = Attrition, y = fct_reorder(feature, desc(Attrition)))) +
  geom_point() +
  # Positive Correlations - Contribute to churn
  geom_segment(aes(xend = 0, yend = feature), 
               color = palette_light()[[2]], 
               data = corrr_analysis %>% filter(Attrition > 0)) +
  geom_point(color = palette_light()[[2]], 
             data = corrr_analysis %>% filter(Attrition > 0)) +
  # Negative Correlations - Prevent churn
  geom_segment(aes(xend = 0, yend = feature), 
               color = palette_light()[[1]], 
               data = corrr_analysis %>% filter(Attrition < 0)) +
  geom_point(color = palette_light()[[1]], 
             data = corrr_analysis %>% filter(Attrition < 0)) +
  # Vertical lines
  geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +
  geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
  geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
  # Aesthetics
  theme_tq() +
  labs(title = "Churn Correlation Analysis",
       subtitle = paste("Positive Correlations (contribute to attrition),",
                        "Negative Correlations (prevent attrition)"),
       y = "Feature Importance")
```

The correlation analysis helps us quickly disseminate which features that the LIME analysis may be excluding. We can see that the following features are highly correlated (magnitude > 0.25):

Increases Likelihood of Attrition (Red):
  - Total Working Years
  - Stock Option level = 1 (true)
  - years in current Role
  - Age
  - Monthly Income

Decreases Likelihood of Attrition (Black):
  - Martial Status = Single
  - Over Time = Yes
  
# Conclusion

In this case i would use specific question is, How to classify using neural network method and how is result of our model and predict classify and compare with our data test. Beside it we would try to uncover which feature 

The result evaluation we use Confussion Matrix as model evaluation and result is :

```{r}
# metrics summary
pred_test %>%
  summarise(
    accuracy = accuracy_vec(Attrition, pred_class),
    sensitivity = sens_vec(Attrition, pred_class),
    specificity = spec_vec(Attrition, pred_class),
    precision = precision_vec(Attrition, pred_class)
  )
```

We can conclude our neural network working not good in this case, our model only good to predict "No" label which mean we cant anticipate employee will have Attrition. Several reason make the Neural Network cant running well using this data but major reason is Total observent its too small, and the data we get is imbalance.

Therefore we can uncover which feature or variable have important to HR understand why employee get Attrition. Based on Intepret in model Audit we can conclude:    

  - Total Working Years
  - Stock Option level
  - years in current Role
  - Age
  - Monthly Income
  - Martial Status
  - Over Time
  
Is variable or feature important to understand employee attrition
  
